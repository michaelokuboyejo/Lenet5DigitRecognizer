{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like to declare constants to avoid typos later on especially if they'd be used in more than one place\n",
    "ReluActivation = 'relu'\n",
    "SoftmaxActivation = 'softmax'\n",
    "AdamOptimizer = 'adam'\n",
    "CategoricalCrossEntropy = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_sets():\n",
    "    path = './data'\n",
    "    print('fetching test and train datasets from kaggle . . .')\n",
    "    url = 'https://www.kaggle.com/c/3004/download-all'\n",
    "    train_set_csv_path = os.path.join(path, 'train.csv')\n",
    "    test_set_csv_path = os.path.join(path, 'test.csv')\n",
    "    zipped_file_path = os.path.join(path, 'data.zip')\n",
    "    if os.path.exists(train_set_csv_path) and os.path.exists(test_set_csv_path):\n",
    "        return\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, zipped_file_path)\n",
    "    zf = ZipFile(zipped_file_path, 'r')\n",
    "    zf.extractall(path=path)\n",
    "    zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching test and train datasets from kaggle . . .\n"
     ]
    }
   ],
   "source": [
    "fetch_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('./data/train.csv')\n",
    "test_set = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = training_set[['label']]\n",
    "\n",
    "X_train = training_set.drop(training_set.columns[[0]], axis=1)\n",
    "X_test = test_set\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Reshape the training and test set\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Padding the images by 2 pixels since in the paper input images were 32x32\n",
    "X_train = np.pad(X_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "X_test = np.pad(X_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "\n",
    "# Standardization\n",
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)\n",
    "X_train = (X_train - mean_px) / std_px\n",
    "\n",
    "Y_train = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=6, kernel_size=5, strides=1, activation=ReluActivation,\n",
    "                 input_shape=(32, 32, 1)))  # The original paper used sinh/tanh activation\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))  # The original paper used Mean/Average pooling\n",
    "model.add(Conv2D(filters=16, kernel_size=5, strides=1, activation=ReluActivation, input_shape=(14, 14, 6)))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Now, the fully connected layers\n",
    "model.add(Dense(units=120, activation=ReluActivation))\n",
    "model.add(Dense(units=84, activation=ReluActivation))\n",
    "\n",
    "# Adding the softmax which did not originally exist in the paper\n",
    "model.add(Dense(units=10, activation=SoftmaxActivation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/42\n",
      "10/10 [==============================] - 57s 6s/step - loss: 1.9902 - accuracy: 0.4265\n",
      "Epoch 2/42\n",
      "10/10 [==============================] - 62s 6s/step - loss: 1.0294 - accuracy: 0.7319\n",
      "Epoch 3/42\n",
      "10/10 [==============================] - 51s 5s/step - loss: 0.4433 - accuracy: 0.8697\n",
      "Epoch 4/42\n",
      "10/10 [==============================] - 50s 5s/step - loss: 0.2921 - accuracy: 0.9122\n",
      "Epoch 5/42\n",
      "10/10 [==============================] - 55s 6s/step - loss: 0.2177 - accuracy: 0.9355\n",
      "Epoch 6/42\n",
      "10/10 [==============================] - 54s 5s/step - loss: 0.1676 - accuracy: 0.9504\n",
      "Epoch 7/42\n",
      "10/10 [==============================] - 53s 5s/step - loss: 0.1319 - accuracy: 0.9611\n",
      "Epoch 8/42\n",
      "10/10 [==============================] - 55s 6s/step - loss: 0.1072 - accuracy: 0.9684\n",
      "Epoch 9/42\n",
      "10/10 [==============================] - 49s 5s/step - loss: 0.0902 - accuracy: 0.9740\n",
      "Epoch 10/42\n",
      "10/10 [==============================] - 48s 5s/step - loss: 0.0780 - accuracy: 0.9775\n",
      "Epoch 11/42\n",
      " 5/10 [==============>...............] - ETA: 25s - loss: 0.0709 - accuracy: 0.9794"
     ]
    }
   ],
   "source": [
    "# compile and fit\n",
    "model.compile(optimizer=AdamOptimizer, loss=CategoricalCrossEntropy, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, steps_per_epoch=10, epochs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction usin the model we just trained\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "index = np.arange(1, 28001)\n",
    "\n",
    "labels = labels.reshape([len(labels), 1])\n",
    "index = index.reshape([len(index), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction to file\n",
    "solution_data_frame = pd.DataFrame()\n",
    "solution_data_frame['ImageId'] = index\n",
    "solution_data_frame['Label'] = labels\n",
    "\n",
    "solution_data_frame.to_csv('./data/solution.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
